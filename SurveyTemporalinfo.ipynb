{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfyabJcm4Gfh8HvRCU/QPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beatriceyapsm/temporaltest/blob/main/SurveyTemporalinfo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67pXm3Dj3fGC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Temporal Information Extraction\n",
        "Temporal information can be represented as {T, E, R}, where T denotes the temporal points, durations or intervals, E means the events, and R represents the temporal relation. \n",
        "Three main approaches to the task of temporal information extraction: rule-based, datadriven, and hybrid.\n",
        "Tempeval-1, Tempeval-2 and Tempeval-3 are all exercises in temporal information extraction. \n",
        "\n",
        "TimeML is a set of rules for encoding documents electronically.\n",
        "- EVENT tag is used to annotate those elements in a text that mark the semantic events \n",
        "- TIMEX3 tag is primarily used to mark up explicit temporal expressions, such as times, dates, durations, etc. \n",
        "- MAKEINSTANCE tag: for when a new instance should be created as an event occurs on multiple days (eg. He taught last Wednesday and today.)\n",
        "- TLINK or Temporal Link represents the temporal relationship holding\n",
        "between events, times, or between an event and a time.\n",
        "\n",
        "Best f1-score for timex3: rule-based system Heideltime\n",
        "Best f1-score for event & makeinstance: ATT-1 Using Max Entropy\n",
        "\n",
        "Publicly available dataset, namely TimeBank.\n",
        "\n",
        "In [51], a method for extracting temporal relations between two\n",
        "events was proposed. It had two stages: (1) a machine-learning model for classifying event attributes (i.e., tense, aspect, modality, polarity, and event class), and (2) a machine-learning model for classifying the\n",
        "relation types between two events. It used TimeBank for experiments, and reported that Naive Bayes (NB) generally gives better performance than maximum entropy (ME). // https://aclanthology.org/P07-2044/\n",
        "\n",
        "In [61], a new corpus for the task of extraction of temporal expressions, namely WikiWars, was introduced. \n",
        "\n",
        "In SemEval-2018, ‘Task 6: Parsing Time Normalizations’ was held as a shared task related to time information extraction [93]. // https://aclanthology.org/D10-1089/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aA4a-21738xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step One: Import nltk and download necessary packages\n",
        " \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        " "
      ],
      "metadata": {
        "id": "ZyJ5yDqjh04Q",
        "outputId": "b82eefda-d445-4735-ce94-1009f8c36621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE WASHINGTON\n",
            "GPE New York\n",
            "PERSON Loretta E. Lynch\n",
            "GPE Brooklyn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step Two: Load Data\n",
        " \n",
        "sentence = \"The European authorities fined Google a record $5.1 Billion on Wednesday this week for abusing its power in the mobile phone market and ordered the company to alter its practices. Coming July, on the third week, the court hearing is scheduled at 9AM. And another meeting will be scheduled on 20th August. Let us discuss about this tomorrow morning at 10AM.\"\n",
        "\n",
        "# Step Three: Tokenise, find parts of speech and chunk words \n",
        "\n",
        "for sent in nltk.sent_tokenize(sentence):\n",
        "  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "     if hasattr(chunk, 'label'):\n",
        "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "id": "AzMKazdUiz_7",
        "outputId": "83910480-a7ed-492a-8f20-0a93c3253663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE European\n",
            "PERSON Google\n"
          ]
        }
      ]
    }
  ]
}